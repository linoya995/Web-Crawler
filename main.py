import reimport requestsimport sysfrom bs4 import BeautifulSoupSAVE_FOLDER = '/home/linoy/Downloads/'  # directory in which files will be downloadeddef join_paths(a, b):    url = a+b    url = url.replace("\\", "/")    return urldef create_soap(website_url):    req = requests.get(website_url)    #req.raise_for_status()                          # raise an exception for error codes    if req.status_code ==404:        print("404 Client Error: Not Found for url")    html_doc = req.text    soup = BeautifulSoup(html_doc, 'html.parser')    return soupdef collect_data(soup, website_url):    device_name_list = []    version_list = []    build_date_list = []    firmware_download_links = soup.find_all('td', class_="views-field views-field-title")    for i in firmware_download_links: device_name_list.append(i.text)    for i in soup.find_all('td', class_="views-field views-field-field-android-version2"): version_list.append(i.text)    for link in firmware_download_links:        firmware_link = join_paths(website_url, link.find('a').get('href'))        soup = create_soap(firmware_link)        for i in soup.find_all('div',                               {'class': lambda x: x and "field-name-changed-date" in x.split()}):            date = i.find('div', class_="field-item even").text            build_date_list.append(date)    for i in range(0,len(device_name_list)):        print("---------" + str(i) + "---------")        print(device_name_list[i])        print(version_list[i])        print(build_date_list[i])def crawler(url):    result = []    device_id = 1    soup = create_soap(url)    # looping through paging    while(True):        # find firmware device link        firmware_download_links = soup.find_all('td', class_="views-field views-field-title")#name and link        firmware_download_links_version = soup.find_all('td', class_="views-field views-field-field-android-version2") #version        # looping through firmware table link        for i in range(0,len(firmware_download_links)):            dict = {}            # find device name            device_name = firmware_download_links[i].text            # find ur news            url_device = firmware_download_links[i].find('a').get('href')            # find version            device_version = firmware_download_links_version[i].text            firmware_link = join_paths("https://www.rockchipfirmware.com/", url_device)            soup2 = create_soap(firmware_link)            date = soup2.find('div',{'class': lambda x: x and "field-name-changed-date" in x.split()})            if date is not None: date = date.find('div', class_="field-item even").text            download_link = soup2.find('a',{'href': lambda x:  x and ".zip" in x} )            if download_link is not None:                print(download_link.text)                download_link_ = download_link.get('href')                download_name = download_link.text            # wrap in dictionary            dict['id'] = device_id            dict['url'] = url_device            dict['version'] = device_version            dict['name'] = device_name            dict['date'] = date            dict['download_link'] = download_link            dict['download_name'] = download_name            print(dict)            result.append(dict)            device_id = device_id+1        # find nextPage        next_page = (soup.find('a', text="next"))        print(next_page)        if next_page is None: break        soup = create_soap(join_paths("https://www.rockchipfirmware.com/",  next_page.get('href')))    return resultdef download_file(file_link, folder,download_name):    file = requests.get(file_link).content    name = file_link.split('/')[-1]    save_path = folder + download_name    print("Saving file:", save_path)    with open(save_path, 'wb') as fp:        fp.write(file)def main(argv):    website_url = argv[0] # url - to start crawling from    soup = create_soap(website_url)    # link to firmware download page    firmware_download_link = soup.find_all('a', title="Download")[0].get('href')    link = join_paths(website_url, firmware_download_link)    crawler(link)    """"    soup = create_soap(link)    collect_data(soup, website_url)    """if __name__ == "__main__":    main(sys.argv[1:])